{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "In this project, our aim is to building a model for predicting dimond prices. Our label (output) will be `price` column. **Do not forget, this is a Classification problem!**\n",
    "\n",
    "## Content\n",
    "carat: weight of the diamond (0.2--5.01)\n",
    "\n",
    "cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "\n",
    "color: diamond colour, from J (worst) to D (best)\n",
    "\n",
    "clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "\n",
    "x: length in mm (0--10.74)\n",
    "\n",
    "y: width in mm (0--58.9)\n",
    "\n",
    "z: depth in mm (0--31.8)\n",
    "\n",
    "depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "\n",
    "table: width of top of diamond relative to widest point (43--95)\n",
    "\n",
    "## Steps\n",
    "- Read the `diamonds.csv` file and describe it.\n",
    "- Make at least 4 different analysis on Exploratory Data Analysis section.\n",
    "- Pre-process the dataset to get ready for ML application. (Check missing data and handle them, can we need to do scaling or feature extraction etc.)\n",
    "- Define appropriate evaluation metric for our case (classification). *Hint: Is there any imbalanced problem in the label column?*\n",
    "- Split the dataset into train and test set. (Consider the imbalanced problem if is there any). Check the distribution of labels in the subsets (train and test).\n",
    "- Train and evaluate Decision Trees and at least 2 different appropriate algorithm which you can choose from scikit-learn library.\n",
    "- Is there any overfitting and underfitting? Interpret your results and try to overcome if there is any problem in a new section.\n",
    "- Create confusion metrics for each algorithm and display Accuracy, Recall, Precision and F1-Score values.\n",
    "- Analyse and compare results of 3 algorithms.\n",
    "- Select best performing model based on evaluation metric you chose on test dataset.\n",
    "\n",
    "\n",
    "Good luck :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Orçun Şimşek</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "data=pd.read_csv(\"diamonds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53940 entries, 0 to 53939\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  53940 non-null  int64  \n",
      " 1   carat       53940 non-null  float64\n",
      " 2   cut         53940 non-null  object \n",
      " 3   color       53940 non-null  object \n",
      " 4   clarity     53940 non-null  object \n",
      " 5   depth       53940 non-null  float64\n",
      " 6   table       53940 non-null  float64\n",
      " 7   price       53940 non-null  int64  \n",
      " 8   x           53940 non-null  float64\n",
      " 9   y           53940 non-null  float64\n",
      " 10  z           53940 non-null  float64\n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Describe our data for each feature and use .info() for get information about our dataset\n",
    "# Analyse missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  carat      cut color clarity  depth  table  price     x     y  \\\n",
       "0           1   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98   \n",
       "1           2   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84   \n",
       "2           3   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07   \n",
       "3           4   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23   \n",
       "4           5   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35   \n",
       "\n",
       "      z  \n",
       "0  2.43  \n",
       "1  2.31  \n",
       "2  2.31  \n",
       "3  2.63  \n",
       "4  2.75  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "carat         0\n",
       "cut           0\n",
       "color         0\n",
       "clarity       0\n",
       "depth         0\n",
       "table         0\n",
       "price         0\n",
       "x             0\n",
       "y             0\n",
       "z             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fair</th>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "      <td>1610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "      <td>4906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ideal</th>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "      <td>21551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Premium</th>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "      <td>13791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very Good</th>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "      <td>12082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0  carat  color  clarity  depth  table  price      x  \\\n",
       "cut                                                                        \n",
       "Fair             1610   1610   1610     1610   1610   1610   1610   1610   \n",
       "Good             4906   4906   4906     4906   4906   4906   4906   4906   \n",
       "Ideal           21551  21551  21551    21551  21551  21551  21551  21551   \n",
       "Premium         13791  13791  13791    13791  13791  13791  13791  13791   \n",
       "Very Good       12082  12082  12082    12082  12082  12082  12082  12082   \n",
       "\n",
       "               y      z  \n",
       "cut                      \n",
       "Fair        1610   1610  \n",
       "Good        4906   4906  \n",
       "Ideal      21551  21551  \n",
       "Premium    13791  13791  \n",
       "Very Good  12082  12082  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our label Distribution (countplot)\n",
    "data.groupby(by=\"cut\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orcun\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='table', ylabel='Density'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example EDA (distplot)\n",
    "sns.distplot(data[\"table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "- Are there any duplicated values?\n",
    "- Do we need to do feature scaling?\n",
    "- Do we need to generate new features?\n",
    "- Split dataset into train and test sets. (0.7/0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53940 entries, 0 to 53939\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  53940 non-null  int64  \n",
      " 1   carat       53940 non-null  float64\n",
      " 2   cut         53940 non-null  object \n",
      " 3   color       53940 non-null  object \n",
      " 4   clarity     53940 non-null  object \n",
      " 5   depth       53940 non-null  float64\n",
      " 6   table       53940 non-null  float64\n",
      " 7   price       53940 non-null  int64  \n",
      " 8   x           53940 non-null  float64\n",
      " 9   y           53940 non-null  float64\n",
      " 10  z           53940 non-null  float64\n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2350"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "z = np.abs(stats.zscore(data.loc[:, data.describe().columns]))\n",
    "z\n",
    "\n",
    "outliers = list(set(np.where(z > 3)[0]))\n",
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>J</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>336</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51585</th>\n",
       "      <td>53935</td>\n",
       "      <td>53936</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51586</th>\n",
       "      <td>53936</td>\n",
       "      <td>53937</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51587</th>\n",
       "      <td>53937</td>\n",
       "      <td>53938</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51588</th>\n",
       "      <td>53938</td>\n",
       "      <td>53939</td>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51589</th>\n",
       "      <td>53939</td>\n",
       "      <td>53940</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51590 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  Unnamed: 0  carat        cut color clarity  depth  table  price  \\\n",
       "0          0           1   0.23      Ideal     E     SI2   61.5   55.0    326   \n",
       "1          1           2   0.21    Premium     E     SI1   59.8   61.0    326   \n",
       "2          3           4   0.29    Premium     I     VS2   62.4   58.0    334   \n",
       "3          4           5   0.31       Good     J     SI2   63.3   58.0    335   \n",
       "4          5           6   0.24  Very Good     J    VVS2   62.8   57.0    336   \n",
       "...      ...         ...    ...        ...   ...     ...    ...    ...    ...   \n",
       "51585  53935       53936   0.72      Ideal     D     SI1   60.8   57.0   2757   \n",
       "51586  53936       53937   0.72       Good     D     SI1   63.1   55.0   2757   \n",
       "51587  53937       53938   0.70  Very Good     D     SI1   62.8   60.0   2757   \n",
       "51588  53938       53939   0.86    Premium     H     SI2   61.0   58.0   2757   \n",
       "51589  53939       53940   0.75      Ideal     D     SI2   62.2   55.0   2757   \n",
       "\n",
       "          x     y     z  \n",
       "0      3.95  3.98  2.43  \n",
       "1      3.89  3.84  2.31  \n",
       "2      4.20  4.23  2.63  \n",
       "3      4.34  4.35  2.75  \n",
       "4      3.94  3.96  2.48  \n",
       "...     ...   ...   ...  \n",
       "51585  5.75  5.76  3.50  \n",
       "51586  5.69  5.75  3.61  \n",
       "51587  5.66  5.68  3.56  \n",
       "51588  6.15  6.12  3.74  \n",
       "51589  5.83  5.87  3.64  \n",
       "\n",
       "[51590 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = data.drop(outliers,axis = 0).reset_index(drop = False)\n",
    "display(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_data.iloc[:,1:-1]\n",
    "y = new_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51590 entries, 0 to 51589\n",
      "Data columns (total 23 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    51590 non-null  int64  \n",
      " 1   carat         51590 non-null  float64\n",
      " 2   cut           51590 non-null  int64  \n",
      " 3   depth         51590 non-null  float64\n",
      " 4   table         51590 non-null  float64\n",
      " 5   price         51590 non-null  int64  \n",
      " 6   x             51590 non-null  float64\n",
      " 7   y             51590 non-null  float64\n",
      " 8   color_D       51590 non-null  uint8  \n",
      " 9   color_E       51590 non-null  uint8  \n",
      " 10  color_F       51590 non-null  uint8  \n",
      " 11  color_G       51590 non-null  uint8  \n",
      " 12  color_H       51590 non-null  uint8  \n",
      " 13  color_I       51590 non-null  uint8  \n",
      " 14  color_J       51590 non-null  uint8  \n",
      " 15  clarity_I1    51590 non-null  uint8  \n",
      " 16  clarity_IF    51590 non-null  uint8  \n",
      " 17  clarity_SI1   51590 non-null  uint8  \n",
      " 18  clarity_SI2   51590 non-null  uint8  \n",
      " 19  clarity_VS1   51590 non-null  uint8  \n",
      " 20  clarity_VS2   51590 non-null  uint8  \n",
      " 21  clarity_VVS1  51590 non-null  uint8  \n",
      " 22  clarity_VVS2  51590 non-null  uint8  \n",
      "dtypes: float64(5), int64(3), uint8(15)\n",
      "memory usage: 3.9 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "new_X = X.copy()\n",
    "new_X['cut'] = new_X['cut'].map({'Fair':1, 'Good': 2, 'Very Good': 3, 'Premium':4, 'Ideal':5})\n",
    "new_X = pd.get_dummies(data = new_X, columns = ['color','clarity'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "categories = encoder.inverse_transform([0,1,2,3,4])\n",
    "\n",
    "new_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Application\n",
    "\n",
    "- Define models.\n",
    "- Fit models.\n",
    "- Evaluate models for both train and test dataset.\n",
    "- Generate Confusion Matrix and scores of Accuracy, Recall, Precision and F1-Score.\n",
    "- Analyse occurrence of overfitting and underfitting. If there is any of them, try to overcome it within a different section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_dtc = DecisionTreeClassifier(random_state=42)\n",
    "model_dtc.fit(X_train_scaled, y_train)\n",
    "\n",
    "train_score = model_dtc.score(X_train_scaled,y_train)\n",
    "test_score = model_dtc.score(X_test_scaled,y_test)\n",
    "\n",
    "pred = model_dtc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train: 1.0\n",
      "Accuracy of test: 0.40712024294113847\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of train:\",train_score)\n",
    "print(\"Accuracy of test:\",test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00         2\n",
      "          11       0.00      0.00      0.00         1\n",
      "          14       0.50      0.67      0.57         3\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       1.00      0.25      0.40         4\n",
      "          17       0.33      0.25      0.29         4\n",
      "          18       0.43      0.60      0.50         5\n",
      "          19       0.44      0.50      0.47         8\n",
      "          20       0.33      0.64      0.44        11\n",
      "          21       0.31      0.22      0.26        18\n",
      "          22       0.27      0.14      0.18        22\n",
      "          23       0.33      0.20      0.25        15\n",
      "          24       0.18      0.33      0.24        12\n",
      "          25       0.39      0.46      0.42        26\n",
      "          26       0.38      0.44      0.41        27\n",
      "          27       0.21      0.30      0.25        10\n",
      "          28       0.56      0.39      0.46        23\n",
      "          29       0.44      0.27      0.33        15\n",
      "          30       0.29      0.43      0.34        14\n",
      "          31       0.38      0.30      0.33        27\n",
      "          32       0.25      0.21      0.23        19\n",
      "          33       0.39      0.46      0.42        26\n",
      "          34       0.33      0.30      0.32        23\n",
      "          35       0.31      0.32      0.32        28\n",
      "          36       0.25      0.29      0.27        24\n",
      "          37       0.31      0.27      0.29        30\n",
      "          38       0.41      0.30      0.35        30\n",
      "          39       0.38      0.36      0.37        28\n",
      "          40       0.45      0.54      0.49        46\n",
      "          41       0.44      0.51      0.47        61\n",
      "          42       0.64      0.64      0.64       102\n",
      "          43       0.62      0.53      0.57       132\n",
      "          44       0.62      0.65      0.63       162\n",
      "          45       0.71      0.69      0.70       192\n",
      "          46       0.71      0.71      0.71       197\n",
      "          47       0.69      0.74      0.71       232\n",
      "          48       0.66      0.61      0.63       234\n",
      "          49       0.61      0.61      0.61       231\n",
      "          50       0.61      0.65      0.63       228\n",
      "          51       0.64      0.59      0.61       192\n",
      "          52       0.60      0.57      0.59       174\n",
      "          53       0.58      0.64      0.61       134\n",
      "          54       0.64      0.57      0.60       114\n",
      "          55       0.60      0.60      0.60       117\n",
      "          56       0.54      0.52      0.53        96\n",
      "          57       0.41      0.57      0.48        76\n",
      "          58       0.51      0.51      0.51        80\n",
      "          59       0.51      0.48      0.49        81\n",
      "          60       0.47      0.41      0.43        69\n",
      "          61       0.44      0.45      0.45        71\n",
      "          62       0.40      0.33      0.36        64\n",
      "          63       0.40      0.43      0.41        53\n",
      "          64       0.35      0.40      0.38        57\n",
      "          65       0.41      0.40      0.41        75\n",
      "          66       0.36      0.38      0.37        71\n",
      "          67       0.42      0.40      0.41        70\n",
      "          68       0.41      0.40      0.40        88\n",
      "          69       0.46      0.48      0.47        83\n",
      "          70       0.49      0.46      0.47        90\n",
      "          71       0.58      0.61      0.60       115\n",
      "          72       0.60      0.52      0.56       110\n",
      "          73       0.68      0.64      0.66       139\n",
      "          74       0.56      0.66      0.61       116\n",
      "          75       0.46      0.50      0.48       111\n",
      "          76       0.60      0.61      0.60       114\n",
      "          77       0.51      0.47      0.49        83\n",
      "          78       0.46      0.47      0.47        66\n",
      "          79       0.38      0.37      0.38        62\n",
      "          80       0.44      0.39      0.42        51\n",
      "          81       0.37      0.30      0.33        33\n",
      "          82       0.31      0.32      0.31        25\n",
      "          83       0.27      0.30      0.29        23\n",
      "          84       0.37      0.50      0.42        28\n",
      "          85       0.17      0.21      0.19        19\n",
      "          86       0.19      0.17      0.18        24\n",
      "          87       0.26      0.17      0.21        29\n",
      "          88       0.29      0.31      0.30        36\n",
      "          89       0.13      0.10      0.11        41\n",
      "          90       0.28      0.28      0.28        39\n",
      "          91       0.42      0.47      0.45        40\n",
      "          92       0.30      0.38      0.33        56\n",
      "          93       0.39      0.36      0.37        87\n",
      "          94       0.37      0.44      0.40       114\n",
      "          95       0.48      0.41      0.44       129\n",
      "          96       0.45      0.43      0.44       121\n",
      "          97       0.45      0.44      0.45       135\n",
      "          98       0.43      0.48      0.45       130\n",
      "          99       0.42      0.38      0.40       130\n",
      "         100       0.42      0.48      0.45       128\n",
      "         101       0.42      0.39      0.41       118\n",
      "         102       0.48      0.43      0.46       106\n",
      "         103       0.43      0.43      0.43        93\n",
      "         104       0.42      0.39      0.41        61\n",
      "         105       0.51      0.48      0.50        77\n",
      "         106       0.41      0.46      0.43        70\n",
      "         107       0.33      0.37      0.35        57\n",
      "         108       0.33      0.30      0.31        70\n",
      "         109       0.35      0.32      0.34        56\n",
      "         110       0.25      0.40      0.31        42\n",
      "         111       0.34      0.31      0.33        42\n",
      "         112       0.15      0.12      0.14        32\n",
      "         113       0.29      0.21      0.24        39\n",
      "         114       0.31      0.27      0.29        41\n",
      "         115       0.28      0.24      0.26        33\n",
      "         116       0.31      0.41      0.35        27\n",
      "         117       0.21      0.17      0.19        35\n",
      "         118       0.27      0.36      0.31        25\n",
      "         119       0.21      0.22      0.21        23\n",
      "         120       0.22      0.22      0.22        27\n",
      "         121       0.25      0.22      0.23        32\n",
      "         122       0.20      0.21      0.21        33\n",
      "         123       0.25      0.18      0.21        38\n",
      "         124       0.25      0.31      0.28        35\n",
      "         125       0.22      0.20      0.21        45\n",
      "         126       0.31      0.35      0.33        48\n",
      "         127       0.26      0.31      0.28        55\n",
      "         128       0.43      0.42      0.43        69\n",
      "         129       0.32      0.32      0.32        81\n",
      "         130       0.42      0.42      0.42        99\n",
      "         131       0.55      0.53      0.54       137\n",
      "         132       0.57      0.54      0.55       141\n",
      "         133       0.46      0.50      0.48       133\n",
      "         134       0.56      0.50      0.53       118\n",
      "         135       0.46      0.47      0.46       101\n",
      "         136       0.36      0.39      0.37        85\n",
      "         137       0.39      0.34      0.36        91\n",
      "         138       0.29      0.33      0.31        82\n",
      "         139       0.22      0.21      0.22        71\n",
      "         140       0.25      0.21      0.23        61\n",
      "         141       0.31      0.40      0.35        55\n",
      "         142       0.34      0.21      0.26        53\n",
      "         143       0.18      0.19      0.18        32\n",
      "         144       0.33      0.40      0.37        42\n",
      "         145       0.29      0.26      0.27        39\n",
      "         146       0.14      0.18      0.16        33\n",
      "         147       0.28      0.32      0.30        28\n",
      "         148       0.24      0.18      0.21        39\n",
      "         149       0.14      0.18      0.16        28\n",
      "         150       0.33      0.40      0.36        35\n",
      "         151       0.26      0.32      0.29        34\n",
      "         152       0.27      0.17      0.21        42\n",
      "         153       0.00      0.00      0.00        25\n",
      "         154       0.32      0.30      0.31        33\n",
      "         155       0.23      0.22      0.22        32\n",
      "         156       0.22      0.20      0.21        44\n",
      "         157       0.34      0.30      0.32        40\n",
      "         158       0.29      0.42      0.35        40\n",
      "         159       0.21      0.22      0.22        50\n",
      "         160       0.25      0.25      0.25        55\n",
      "         161       0.35      0.23      0.27        84\n",
      "         162       0.42      0.35      0.38        94\n",
      "         163       0.37      0.54      0.44        76\n",
      "         164       0.38      0.33      0.35        83\n",
      "         165       0.27      0.31      0.29        78\n",
      "         166       0.38      0.39      0.38        75\n",
      "         167       0.30      0.32      0.31        69\n",
      "         168       0.32      0.31      0.32        77\n",
      "         169       0.25      0.27      0.26        51\n",
      "         170       0.28      0.29      0.29        65\n",
      "         171       0.33      0.33      0.33        79\n",
      "         172       0.36      0.31      0.34        96\n",
      "         173       0.34      0.34      0.34        92\n",
      "         174       0.37      0.39      0.38       103\n",
      "         175       0.34      0.37      0.35       103\n",
      "         176       0.41      0.38      0.39       129\n",
      "         177       0.46      0.41      0.43       163\n",
      "         178       0.46      0.53      0.49       148\n",
      "         179       0.45      0.46      0.46       146\n",
      "         180       0.38      0.42      0.40       163\n",
      "         181       0.36      0.37      0.36       147\n",
      "         182       0.26      0.26      0.26       113\n",
      "         183       0.40      0.32      0.35       123\n",
      "         184       0.35      0.32      0.34        93\n",
      "         185       0.29      0.38      0.33        64\n",
      "         186       0.40      0.36      0.38        74\n",
      "         187       0.40      0.36      0.38        80\n",
      "         188       0.30      0.34      0.32        61\n",
      "         189       0.29      0.29      0.29        70\n",
      "         190       0.25      0.26      0.26        61\n",
      "         191       0.30      0.26      0.28        72\n",
      "         192       0.25      0.32      0.28        47\n",
      "         193       0.29      0.23      0.26        56\n",
      "         194       0.31      0.38      0.34        45\n",
      "         195       0.34      0.22      0.27        59\n",
      "         196       0.30      0.25      0.27        68\n",
      "         197       0.23      0.25      0.24        63\n",
      "         198       0.24      0.30      0.26        54\n",
      "         199       0.37      0.31      0.34        75\n",
      "         200       0.30      0.32      0.31        65\n",
      "         201       0.41      0.38      0.39        82\n",
      "         202       0.30      0.33      0.32        69\n",
      "         203       0.35      0.31      0.33        81\n",
      "         204       0.34      0.36      0.35        72\n",
      "         205       0.26      0.28      0.27        64\n",
      "         206       0.23      0.25      0.24        48\n",
      "         207       0.26      0.25      0.25        40\n",
      "         208       0.26      0.24      0.25        34\n",
      "         209       0.14      0.17      0.15        24\n",
      "         210       0.35      0.22      0.27        41\n",
      "         211       0.20      0.23      0.21        22\n",
      "         212       0.17      0.28      0.21        18\n",
      "         213       0.26      0.27      0.27        22\n",
      "         214       0.27      0.19      0.22        21\n",
      "         215       0.00      0.00      0.00        16\n",
      "         216       0.30      0.37      0.33        27\n",
      "         217       0.15      0.11      0.12        19\n",
      "         218       0.13      0.10      0.11        21\n",
      "         219       0.16      0.19      0.17        16\n",
      "         220       0.36      0.23      0.28        22\n",
      "         221       0.36      0.29      0.32        14\n",
      "         222       0.32      0.35      0.33        20\n",
      "         223       0.27      0.29      0.28        14\n",
      "         224       0.29      0.33      0.31        21\n",
      "         225       0.27      0.17      0.21        24\n",
      "         226       0.18      0.29      0.22        21\n",
      "         227       0.25      0.31      0.28        29\n",
      "         228       0.30      0.29      0.29        35\n",
      "         229       0.23      0.26      0.24        31\n",
      "         230       0.34      0.30      0.32        43\n",
      "         231       0.18      0.24      0.21        46\n",
      "         232       0.21      0.18      0.20        49\n",
      "         233       0.25      0.29      0.27        52\n",
      "         234       0.25      0.25      0.25        61\n",
      "         235       0.41      0.39      0.40        67\n",
      "         236       0.36      0.33      0.34        58\n",
      "         237       0.28      0.32      0.30        53\n",
      "         238       0.20      0.15      0.17        46\n",
      "         239       0.35      0.31      0.33        49\n",
      "         240       0.33      0.42      0.37        38\n",
      "         241       0.24      0.15      0.18        34\n",
      "         242       0.39      0.33      0.36        33\n",
      "         243       0.18      0.31      0.23        13\n",
      "         244       0.16      0.21      0.18        14\n",
      "         245       0.24      0.22      0.23        18\n",
      "         246       0.22      0.25      0.24        16\n",
      "         247       0.31      0.22      0.26        18\n",
      "         248       0.29      0.16      0.21        25\n",
      "         249       0.33      0.39      0.36        18\n",
      "         250       0.17      0.14      0.15        14\n",
      "         251       0.18      0.14      0.16        14\n",
      "         252       0.17      0.25      0.20         8\n",
      "         253       0.19      0.23      0.21        13\n",
      "         254       0.41      0.39      0.40        18\n",
      "         255       0.00      0.00      0.00        11\n",
      "         256       0.07      0.10      0.08        10\n",
      "         257       0.20      0.40      0.27         5\n",
      "         258       0.40      0.55      0.46        11\n",
      "         259       0.00      0.00      0.00         6\n",
      "         260       0.33      0.25      0.29         4\n",
      "         261       0.33      0.20      0.25         5\n",
      "         262       0.33      0.38      0.35         8\n",
      "         263       0.50      0.11      0.18         9\n",
      "         264       0.00      0.00      0.00         3\n",
      "         265       0.33      0.33      0.33         3\n",
      "         266       0.20      0.12      0.15         8\n",
      "         267       0.12      0.17      0.14         6\n",
      "         268       0.00      0.00      0.00         3\n",
      "         269       0.22      0.40      0.29         5\n",
      "         270       0.00      0.00      0.00         9\n",
      "         271       0.18      0.29      0.22         7\n",
      "         272       0.57      0.29      0.38        14\n",
      "         273       0.27      0.33      0.30         9\n",
      "         274       0.20      0.14      0.17         7\n",
      "         275       0.07      0.10      0.08        10\n",
      "         276       0.12      0.20      0.15        10\n",
      "         277       0.67      0.36      0.47        11\n",
      "         278       0.25      0.13      0.17        15\n",
      "         279       0.14      0.22      0.17         9\n",
      "         280       0.18      0.17      0.17        12\n",
      "         281       0.07      0.08      0.08        12\n",
      "         282       0.18      0.25      0.21        12\n",
      "         283       0.17      0.18      0.17        11\n",
      "         284       0.16      0.30      0.21        10\n",
      "         285       0.30      0.21      0.25        14\n",
      "         286       0.40      0.17      0.24        12\n",
      "         287       0.30      0.43      0.35         7\n",
      "         288       0.60      0.30      0.40        10\n",
      "         289       0.20      0.40      0.27         5\n",
      "         290       0.50      0.43      0.46         7\n",
      "         291       0.00      0.00      0.00         6\n",
      "         292       0.29      0.22      0.25         9\n",
      "         293       0.29      0.20      0.24        10\n",
      "         294       0.17      0.33      0.22         3\n",
      "         295       0.00      0.00      0.00         2\n",
      "         296       0.00      0.00      0.00         5\n",
      "         297       0.11      0.50      0.18         2\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         302       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.41     15477\n",
      "   macro avg       0.32      0.31      0.31     15477\n",
      "weighted avg       0.41      0.41      0.41     15477\n",
      "\n",
      "Precision = 0.3151681542311126\n",
      "Recall = 0.3130490462124684\n",
      "Accuracy = 0.40712024294113847\n",
      "F1 Score = 0.30829336179046385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orcun\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orcun\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orcun\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\orcun\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"Precision = {}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"F1 Score = {}\".format(f1_score(y_test, pred,average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orcun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training accuracy: 0.07916806496739373\n",
      "Test accuracy: 0.07818052594171997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "models = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "cv = cross_validate(models,X_train ,y_train, return_estimator=True)\n",
    "\n",
    "print(\"Mean training accuracy: {}\".format(np.mean(cv['test_score'])))\n",
    "print(\"Test accuracy: {}\".format(cv[\"estimator\"][0].score(X_test_scaled ,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orcun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "GridSearchCV(estimator=SVC(),\n",
    "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "- Select the best performing model and write your comments about why choose this model.\n",
    "- Analyse results and make comment about how you can improve model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
